{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "# 5. Gradient Methods\n",
    "\n",
    "This notebook presents gradient methods, useful for learning in some environment with a large state space.\n",
    "\n",
    "We use a neural network with a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TicTacToe, Nim, ConnectFour\n",
    "from agent import Agent, OnlineEvaluation\n",
    "from dynamic import ValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCLearning(OnlineEvaluation):\n",
    "    \"\"\"Online evaluation by Monte-Carlo.\"\"\"\n",
    "        \n",
    "    def update_values(self, state=None, horizon=100):\n",
    "        \"\"\"Update the values from one episode.\"\"\"\n",
    "        stop, states, rewards = self.get_episode(state=state, horizon=horizon)\n",
    "        # remove last state\n",
    "        states.pop()\n",
    "        gain = 0\n",
    "        # backward update\n",
    "        for state, reward in zip(reversed(states), reversed(rewards)):\n",
    "            self.add_state(state)\n",
    "            code = self.model.encode(state)\n",
    "            self.count[code] += 1\n",
    "            # to be modified\n",
    "            # begin\n",
    "            gain = reward + self.gamma * gain\n",
    "            # end \n",
    "            diff = gain - self.value[code]\n",
    "            count = self.count[code]\n",
    "            self.value[code] += diff / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with value-based methods. The neural network is a regressor that approximates the value function. Note that the model is supposed to be known, so that we don't need the action-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    \"\"\"Neural network for value function approximation. Return the value of each state.\"\"\"\n",
    "    def __init__(self, model, hidden_size):\n",
    "        if not hasattr(model, 'one_hot_encode'):\n",
    "            raise ValueError(\"The environment must have a one-hot encoding of states.\")   \n",
    "        super(Regressor, self).__init__()\n",
    "        self.model = model\n",
    "        state = model.init_state()\n",
    "        code = model.one_hot_encode(state)\n",
    "        input_size = len(code)\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size), \n",
    "            nn.GELU(), \n",
    "            nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def forward(self, code):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        return self.nn(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Regressor(model=game, hidden_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.state\n",
    "# one-hot encoding\n",
    "code = game.one_hot_encode(state)\n",
    "# tensor\n",
    "code = torch.tensor(code).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = regressor.forward(code).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1113])\n"
     ]
    }
   ],
   "source": [
    "# value of the state\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Complete the method get_best_actions of the class ValueGradient. \n",
    "* Test the agent on TicTacToe, against (1) a random adversary and (2) a perfect adversary.\n",
    "* Test the agent on ConnectFour, against (1) a random adversary and (2) an adversary with the one-step policy.\n",
    "* Compare your results to another learning strategy (e.g., Monte-Carlo learning) and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueGradient(OnlineEvaluation):\n",
    "    \"\"\"Agent learning by value gradient. The model is supposed to be known.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object of class Environment\n",
    "        Model.\n",
    "    player : int\n",
    "        Player for games (1 or -1, default = default player of the game).\n",
    "    gamma : float\n",
    "        Discount rate (in [0, 1], default = 1).\n",
    "    hidden_size : int\n",
    "        Size of the hidden layer (default = 100).\n",
    "    \"\"\"\n",
    "    def __init__(self, model, player=None, gamma=1, hidden_size=100):\n",
    "        super(ValueGradient, self).__init__(model, player=player)  \n",
    "        if not hasattr(model, \"get_next_state\"):\n",
    "            raise ValueError(\"The model must be known, with a 'get_next_state' method.\")\n",
    "        self.nn = Regressor(model, hidden_size)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_best_actions(self, state):\n",
    "        \"\"\"Get the best actions in some state according to the value function.\"\"\" \n",
    "        actions = self.get_actions(state)\n",
    "        if len(actions) > 1:\n",
    "            # to be modified\n",
    "            # if the state is terminal, take the actual value\n",
    "            values = np.zeros(len(actions))\n",
    "            for i, action in enumerate(actions):\n",
    "                next_state = self.model.get_next_state(state, action)\n",
    "                if self.model.is_terminal(next_state):\n",
    "                    values[i] = self.model.get_reward(next_state)\n",
    "                else:\n",
    "                    code = self.model.one_hot_encode(next_state)\n",
    "                    code = torch.tensor(code).float()\n",
    "                    values[i] = self.nn.forward(code).detach().numpy()[0]\n",
    "            \n",
    "            if self.player == 1:\n",
    "                best_value = max(values)\n",
    "            else:\n",
    "                best_value = min(values)\n",
    "            actions = [action for action, value in zip(actions, values) if value==best_value]\n",
    "        return actions        \n",
    "    \n",
    "    def update_policy(self):\n",
    "        self.policy = self.get_policy()\n",
    "    \n",
    "    def get_samples(self, horizon, epsilon):\n",
    "        \"\"\"Get samples from one episode under the epsilon-greedy policy.\"\"\"\n",
    "        self.policy = self.randomize_policy(epsilon=epsilon)\n",
    "        self.model.reset()\n",
    "        state = self.model.state\n",
    "        states = []\n",
    "        rewards = []\n",
    "        for t in range(horizon):\n",
    "            action = self.get_action(state)\n",
    "            reward, stop = self.model.step(action)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            if stop:\n",
    "                break\n",
    "            state = self.model.state\n",
    "        gains = []\n",
    "        gain = 0\n",
    "        for reward in reversed(rewards):\n",
    "            gain = reward + self.gamma * gain\n",
    "            gains.append(gain)\n",
    "        return reversed(states), gains\n",
    "        \n",
    "    def train(self, horizon=100, n_episodes=1000, learning_rate=0.01, epsilon=0.1):\n",
    "        \"\"\"Train the neural network with samples drawn from the epsilon-greedy policy.\"\"\"\n",
    "        optimizer = optim.Adam(self.nn.parameters(), lr=learning_rate)\n",
    "        for t in range(n_episodes):\n",
    "            states, gains = self.get_samples(horizon, epsilon)\n",
    "            codes = [self.model.one_hot_encode(state) for state in states]\n",
    "            codes = np.array(codes)\n",
    "            codes = torch.tensor(codes).float()\n",
    "            values = self.nn.forward(codes)\n",
    "            gains = torch.tensor(gains).float().reshape(-1, 1)\n",
    "            mse = nn.MSELoss()\n",
    "            loss = mse(values, gains)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TicTacToe, Random (Without training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe()\n",
    "agent = ValueGradient(game)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.unique(gains, return_counts=True), np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TicTacToe, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe()\n",
    "agent = ValueGradient(game)\n",
    "agent.train(n_episodes=1000)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.unique(gains, return_counts=True), np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TicTacToe, Perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_optimal = ValueIteration(game)\n",
    "_, adversary_policy = algo_optimal.get_perfect_players()\n",
    "game = TicTacToe(adversary_policy=adversary_policy)\n",
    "agent = ValueGradient(game)\n",
    "agent.train(n_episodes=6000)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.unique(gains, return_counts=True), np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConnectFour, Random (Without training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFour()\n",
    "agent = ValueGradient(game)\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.unique(gains, return_counts=True), np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConnectFour, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFour()\n",
    "agent = ValueGradient(game)\n",
    "agent.train(n_episodes=100)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.unique(gains, return_counts=True), np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConnectFour, One-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFour(adversary_policy='one_step')\n",
    "agent = ValueGradient(game)\n",
    "agent.train(n_episodes=100)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.unique(gains, return_counts=True), np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between Value Gradient & MC\n",
    "\n",
    "Because the training is slow, you can change the value for the two commented lines for quick review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_optimal = ValueIteration(game)\n",
    "_, adversary_policy = algo_optimal.get_perfect_players()\n",
    "game = TicTacToe(adversary_policy=adversary_policy)\n",
    "agent_vg = ValueGradient(game)\n",
    "algo = MCLearning(game, policy='random')\n",
    "policy = algo.get_policy()\n",
    "agent_mc = Agent(game, policy)\n",
    "iters = [0]\n",
    "gains_mc = [np.mean(agent_mc.get_gains(n_runs=100))]\n",
    "gains_vg = [np.mean(agent_vg.get_gains(n_runs=100))]\n",
    "n_iter = 30 # change this value\n",
    "n_games_per_iter = 100 # change this value\n",
    "for iter in tqdm(range(n_iter)):\n",
    "    iters.append((iter+1)*n_games_per_iter)\n",
    "    for t in range(n_games_per_iter):\n",
    "        algo.update_values(state='random')\n",
    "    policy = algo.get_policy()\n",
    "    agent_mc = Agent(game, policy)\n",
    "    gains_mc.append(np.mean(agent_mc.get_gains(n_runs=100)))\n",
    "    agent_vg.train(n_episodes=n_games_per_iter)\n",
    "    agent_vg.update_policy()\n",
    "    gains_vg.append(np.mean(agent_vg.get_gains(n_runs=100)))\n",
    "\n",
    "plt.plot(iters, gains_mc, '-o', label='Monte-Carlo')\n",
    "plt.plot(iters, gains_vg, '-o', label='Value Gradient')\n",
    "plt.xlabel('Games')\n",
    "plt.ylabel('Gains')\n",
    "plt.ylim(-1.1, 0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value gradient method works, but it converges slower than MC. It's probably because value gradient method relies on an accurate estimation of the gradient direction, and when facing perfect player, the sparsity and instability of the gradient signal can affect the optimization efficiency of the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider a policy-based method. The neural network is a classifier that approximates the optimal policy. It returns the probability of each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Neural network for policy gradient. Return the distribution of actions in each state.\"\"\"\n",
    "    def __init__(self, model, hidden_size):\n",
    "        if not hasattr(model, 'one_hot_encode'):\n",
    "            raise ValueError(\"The environment must have a one-hot encoding of states.\")   \n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = model\n",
    "        actions = model.get_all_actions()\n",
    "        if self.model.is_game():\n",
    "            # remove action when passing\n",
    "            actions.pop()\n",
    "        self.actions = actions\n",
    "        state = model.init_state()\n",
    "        code = model.one_hot_encode(state)\n",
    "        input_size = len(code)\n",
    "        output_size = len(actions)\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size), \n",
    "            nn.GELU(), \n",
    "            nn.Linear(hidden_size, output_size), \n",
    "            nn.Softmax(dim=0))\n",
    "\n",
    "    def forward(self, code):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        return self.nn(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(model=game, hidden_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.state\n",
    "# one-hot encoding\n",
    "code = game.one_hot_encode(state)\n",
    "# tensor\n",
    "code = torch.tensor(code).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = classifier.forward(code).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Complete the method 'train' of the class PolicyGradient. Observe that a penalty is assigned for illegal actions.\n",
    "* Test the agent on TicTacToe, against (1) a random adversary and (2) a perfect adversary.\n",
    "* Test the agent on ConnectFour, against (1) a random adversary and (2) an adversary with the one-step policy.\n",
    "* Compare your results to another learning strategy (e.g., Monte-Carlo learning) and interpret the results.\n",
    "* (bonus) Try to improve policy gradient on TicTacToe with a perfect adversary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient(Agent):\n",
    "    \"\"\"Agent learning by policy gradient.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object of class Environment\n",
    "        Model.a\n",
    "    player : int\n",
    "        Player for games (1 or -1, default = default player of the game).\n",
    "    gamma : float\n",
    "        Discount rate (in [0, 1], default = 1).\n",
    "    hidden_size : int\n",
    "        Size of the hidden layer (default = 100).\n",
    "    penalty : float\n",
    "        Penalty for illegal actions (default = -5).\n",
    "    min_log : float\n",
    "        Minimal value to compute the log (default = 1e-10)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, player=None, gamma=1, hidden_size=100, penalty=-1, min_log=1e-10):\n",
    "        super(PolicyGradient, self).__init__(model, player=player)  \n",
    "        self.nn = Classifier(model, hidden_size)\n",
    "        self.action_id = {action: i for i, action in enumerate(self.nn.actions)}\n",
    "        self.gamma = gamma\n",
    "        self.penalty = penalty\n",
    "        self.min_log = min_log\n",
    "        \n",
    "    def get_policy(self):\n",
    "        \"\"\"Get the current policy.\"\"\"\n",
    "        def policy(state):\n",
    "            actions = self.model.get_actions(state)\n",
    "            if len(actions) > 1:\n",
    "                win_actions = []\n",
    "                # check win actions\n",
    "                if self.model.is_game():\n",
    "                    next_states = [self.model.get_next_state(state, action) for action in actions]\n",
    "                    win_actions = [self.model.get_reward(next_state) == self.player for next_state in next_states]\n",
    "                if any(win_actions):\n",
    "                    probs = np.array(win_actions).astype(float)\n",
    "                else:\n",
    "                    # get prob of each action\n",
    "                    code = self.model.one_hot_encode(state)\n",
    "                    code = torch.tensor(code).float()\n",
    "                    probs = self.nn.forward(code)\n",
    "                    probs = probs.detach().numpy()\n",
    "                    # restrict to available actions\n",
    "                    indices = [self.action_id[action] for action in actions]\n",
    "                    probs = probs[indices]                    \n",
    "                # renormalize\n",
    "                if np.sum(probs) > 0:\n",
    "                    probs = probs / np.sum(probs)\n",
    "                else:\n",
    "                    probs = np.ones(len(actions)) / len(actions)\n",
    "            else:\n",
    "                probs = [1]\n",
    "            return probs, actions\n",
    "        return policy\n",
    "    \n",
    "    def update_policy(self):\n",
    "        \"\"\"Update the policy.\"\"\"\n",
    "        self.policy = self.get_policy()\n",
    "    \n",
    "    def get_samples(self, horizon):\n",
    "        \"\"\"Get samples from one episode.\"\"\"\n",
    "        self.update_policy()\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        log_probs_illegal = []\n",
    "        self.model.reset()\n",
    "        state = self.model.state\n",
    "        for t in range(horizon):\n",
    "            action = self.get_action(state)\n",
    "            if action is not None:\n",
    "                i = self.action_id[action]\n",
    "                code = self.model.one_hot_encode(state)\n",
    "                code = torch.tensor(code).float()\n",
    "                probs = self.nn.forward(code)\n",
    "                prob = torch.clip(probs[i], self.min_log, 1 - self.min_log)\n",
    "                log_prob = torch.log(prob).reshape(1)\n",
    "\n",
    "                actions = self.model.get_actions(state)\n",
    "                if action in actions:\n",
    "                    reward, stop = self.model.step(action)\n",
    "                    state = self.model.state\n",
    "                    rewards.append(reward)\n",
    "                    log_probs.append(log_prob)\n",
    "                else:\n",
    "                    log_probs_illegal.append(log_prob)\n",
    "            else:\n",
    "                reward, stop = self.model.step(action)\n",
    "                rewards.append(reward)\n",
    "                state = self.model.state\n",
    "            if stop:\n",
    "                break\n",
    "        gain = 0\n",
    "        for reward in reversed(rewards):\n",
    "            gain = reward + self.gamma * gain\n",
    "        return gain, log_probs, log_probs_illegal\n",
    "        \n",
    "    def train(self, horizon=100, n_episodes=1000, learning_rate=0.01):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        optimizer = optim.Adam(self.nn.parameters(), lr=learning_rate)\n",
    "        accumulated_gain = 0\n",
    "        for t in range(n_episodes):\n",
    "            gain, log_probs, log_probs_illegal = self.get_samples(horizon)\n",
    "            accumulated_gain += gain\n",
    "            loss = 0\n",
    "            if len(log_probs):\n",
    "                # to be modified\n",
    "                for log_prob in log_probs:\n",
    "                    loss += -log_prob * (gain - accumulated_gain / (t + 1))\n",
    "            if len(log_probs_illegal):\n",
    "                for log_prob in log_probs_illegal:\n",
    "                    loss += self.penalty * log_prob\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TicTacToe, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe()\n",
    "agent = PolicyGradient(game)\n",
    "agent.train(n_episodes=2000)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.unique(gains, return_counts=True), np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TicTacToe, Perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_optimal = ValueIteration(game)\n",
    "_, adversary_policy = algo_optimal.get_perfect_players()\n",
    "game = TicTacToe(adversary_policy=adversary_policy)\n",
    "agent = PolicyGradient(game)\n",
    "agent.train(n_episodes=6000)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.unique(gains, return_counts=True), np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConnectFour, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFour()\n",
    "agent = PolicyGradient(game)\n",
    "agent.train(n_episodes=1000)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.unique(gains, return_counts=True), np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConnectFour, One-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFour(adversary_policy='one_step')\n",
    "agent = PolicyGradient(game)\n",
    "agent.train(n_episodes=2000)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.unique(gains, return_counts=True), np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between Policy Gradient & MC\n",
    "\n",
    "Because the training is slow, you can change the value for the two commented lines for quick review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_optimal = ValueIteration(game)\n",
    "_, adversary_policy = algo_optimal.get_perfect_players()\n",
    "game = TicTacToe(adversary_policy=adversary_policy)\n",
    "agent_pg = PolicyGradient(game)\n",
    "algo = MCLearning(game, policy='random')\n",
    "policy = algo.get_policy()\n",
    "agent_mc = Agent(game, policy)\n",
    "iters = [0]\n",
    "gains_mc = [np.mean(agent_mc.get_gains(n_runs=100))]\n",
    "gains_pg = [np.mean(agent_pg.get_gains(n_runs=100))]\n",
    "n_iter = 30 # change this value\n",
    "n_games_per_iter = 100 # change this value\n",
    "for iter in tqdm(range(n_iter)):\n",
    "    iters.append((iter+1)*n_games_per_iter)\n",
    "    for t in range(n_games_per_iter):\n",
    "        algo.update_values(state='random')\n",
    "    policy = algo.get_policy()\n",
    "    agent_mc = Agent(game, policy)\n",
    "    gains_mc.append(np.mean(agent_mc.get_gains(n_runs=100)))\n",
    "    agent_pg.train(n_episodes=n_games_per_iter)\n",
    "    agent_pg.update_policy()\n",
    "    gains_pg.append(np.mean(agent_pg.get_gains(n_runs=100)))\n",
    "\n",
    "plt.plot(iters, gains_mc, '-o', label='Monte-Carlo')\n",
    "plt.plot(iters, gains_pg, '-o', label='Policy Gradient')\n",
    "plt.xlabel('Games')\n",
    "plt.ylabel('Gains')\n",
    "plt.ylim(-1.1, 0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After optimization, it's better than before and better than MC. In fact, in the previous policy gradient method, there is no explicit use of the value function, possessing an over-reliance on feedback from strategy execution when facing perfect opponents, therefore leading to inefficient learning. But after optimizing with centering gains, it works better because instead of stopping learning because of the low absolute value of the overall reward, it focuses more on the relative differences in rewards and stay motivated to improve the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
