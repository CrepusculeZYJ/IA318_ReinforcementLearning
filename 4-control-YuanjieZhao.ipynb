{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "# 4. Online control\n",
    "\n",
    "This notebook presents the **online control** of an agent by SARSA and Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TicTacToe, Nim, ConnectFour\n",
    "from agent import Agent, OnlineControl\n",
    "from dynamic import ValueIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Complete the class ``SARSA`` and test it on Tic-Tac-Toe.\n",
    "* Complete the class ``QLearning`` and test it on Tic-Tac-Toe.\n",
    "* Compare these algorithms on Tic-Tac-Toe (play first) and Nim (play second), using a random adversary, then a perfect adversary. Comment your results.\n",
    "* Test these algorithms on Connect 4 against a random adversary. Comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(OnlineControl):\n",
    "    \"\"\"Online control by SARSA.\"\"\"\n",
    "        \n",
    "    def update_values(self, state=None, horizon=100, epsilon=0.5):\n",
    "        \"\"\"Learn the action-value function online.\"\"\"\n",
    "        self.model.reset(state)\n",
    "        state = self.model.state\n",
    "        if not self.model.is_terminal(state):\n",
    "            action = self.randomize_best_action(state, epsilon=epsilon)\n",
    "            for t in range(horizon):\n",
    "                code = self.model.encode(state)\n",
    "                self.action_count[code][action] += 1\n",
    "                reward, stop = self.model.step(action)\n",
    "                # to be modified (get sample gain)\n",
    "                # begin\n",
    "                next_state = self.model.state\n",
    "                next_code = self.model.encode(next_state)\n",
    "                if not stop:\n",
    "                    next_action = self.randomize_best_action(next_state, epsilon=epsilon)\n",
    "                    gain = reward + self.gamma * self.action_value[next_code][next_action]\n",
    "                else:\n",
    "                    gain = reward\n",
    "                # end\n",
    "                diff = gain - self.action_value[code][action]\n",
    "                count = self.action_count[code][action]\n",
    "                self.action_value[code][action] += diff / count\n",
    "                if stop:\n",
    "                    break\n",
    "                # to be modified (update state and action)\n",
    "                # begin\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                # end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(OnlineControl):\n",
    "    \"\"\"Online control by Q-learning.\"\"\"\n",
    "        \n",
    "    def update_values(self, state=None, horizon=100, epsilon=0.5):\n",
    "        \"\"\"Learn the action-value function online.\"\"\"\n",
    "        self.model.reset(state)\n",
    "        state = self.model.state\n",
    "        # to be completed\n",
    "        if not self.model.is_terminal(state):\n",
    "            for t in range(horizon):\n",
    "                code = self.model.encode(state)\n",
    "                action = self.randomize_best_action(state, epsilon=epsilon)\n",
    "                self.action_count[code][action] += 1\n",
    "                reward, stop = self.model.step(action)\n",
    "                next_state = self.model.state\n",
    "                next_code = self.model.encode(next_state)\n",
    "                if not stop:\n",
    "                    best_action = self.get_best_actions(next_state)[0]\n",
    "                    gain = reward + self.gamma * self.action_value[next_code][best_action]\n",
    "                else:\n",
    "                    gain = reward\n",
    "                diff = gain - self.action_value[code][action]\n",
    "                count = self.action_count[code][action]\n",
    "                self.action_value[code][action] += diff / count\n",
    "                if stop:\n",
    "                    break\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TicTacToe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.301"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game()\n",
    "agent = Agent(game)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.814"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game()\n",
    "agent = Agent(game)\n",
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perfect adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()\n",
    "algo_optimal = ValueIteration(game)\n",
    "_, adversary_policy = algo_optimal.get_perfect_players()\n",
    "game = Game(adversary_policy=adversary_policy)\n",
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()\n",
    "agent = Agent(game)\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perfect adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()\n",
    "algo_optimal = ValueIteration(game)\n",
    "_, adversary_policy = algo_optimal.get_perfect_players()\n",
    "game = Game(adversary_policy=adversary_policy)\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = Nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(play_first=False)\n",
    "agent = Agent(game)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.582"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(play_first=False)\n",
    "agent = Agent(game)\n",
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perfect adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.314"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(play_first=False)\n",
    "algo_optimal = ValueIteration(game)\n",
    "_, adversary_policy = algo_optimal.get_perfect_players()\n",
    "game = Game(adversary_policy=adversary_policy, play_first=False)\n",
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.694"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(play_first=False)\n",
    "agent = Agent(game)\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perfect adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(play_first=False)\n",
    "algo_optimal = ValueIteration(game)\n",
    "_, adversary_policy = algo_optimal.get_perfect_players()\n",
    "game = Game(adversary_policy=adversary_policy, play_first=False)\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: It seems that Q-Learning performs better than SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = ConnectFour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()\n",
    "agent = Agent(game)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.138"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game()\n",
    "agent = Agent(game)\n",
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()\n",
    "agent = Agent(game)\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: As usual, Q-Learning performs better than SARSA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
